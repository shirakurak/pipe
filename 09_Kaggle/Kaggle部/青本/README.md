# [Python機械学習プログラミング 達人データサイエンティストによる理論と実践](https://book.impress.co.jp/books/1120101017)

リポジトリ：[Python Machine Learning (3rd Ed.) Code Repository](https://github.com/rasbt/python-machine-learning-book-3rd-edition)

## 第1章 「データから学習する能力」をコンピュータに与える

### 1.1 データを知識に変える「知能機械」

- 機械学習 ⊂ 人工知能
- データ → （自ら学習するアルゴリズム） → 予測を行うための知識

日常生活における機械学習の役割

- 堅牢なメールスパムフィルタ
- 文字/音声認識ソフトウェア
- 信頼性の高いWeb検索エンジン

それ以外のディープラーニングの成果

- 皮膚がんの検出
- タンパク質の立体構造を予測する

### 1.2 3種類の機械学習

- 教師あり学習（supervised learning）
- 教師なし学習（unsupervised learning）
- 強化学習（reinforcement learining）

#### 1.2.1 「教師あり学習」による未来予測

- 主な目標
  - ラベル付けされた訓練データからモデルを学習 → 未知のデータや将来のデータを予測できるようにする
- 教師（あり） = 望ましいラベルが判明している訓練データ 

例：メールスパムフィルタ（スパムメールとそれ以外を分類）

- 分類（classification）：離散値のクラスラベルを持つ
- 回帰（regression）：連続値のクラスラベルを持つ

**クラスラベルを予測するための分類**

- 二値分類（binary classification）
- 多クラス分類（multiclass classification）
  - 例：手描き文字認識

**連続値を予測するための回帰**

- 回帰分析（regression analysis）とも呼ばれる
- 複数の予測変数（特徴量ともいう）と連続値の応答変数（目的変数ともいう）が与えられ、それらの関係を探る

例：試験勉強に費やした時間と最終的な点数との間の関係を学習し、次の受験生に対する点数を予測するモデルを作る

#### 1.2.2 強化学習による対話問題の解決

- 目的：エージェント（環境とのやり取りに基づいて性能を改善するシステム）を作ること
- 環境とのやりとりを通じて、報酬を最大化しようとする

例：チェスエンジン

#### 1.2.3 「教師なし学習」による隠れた構造の発見

扱うのは

- ラベル付けされていないデータ
- 構造が不明な（unknown structure）データ

**クラスタリングによるグループの発見**

**データ圧縮のための次元削減**

特徴量の前処理においてよく使われるアプローチの1つ

- データからノイズを取り除く
- 関連する大半の情報を維持した上で、データをより低い次元の部分空間に圧縮する

### 1.3 基本用語と表記法

#### 1.3.1 本書の表記と規約

#### 1.3.2 機械学習の用語

- 訓練データ（training example）
- 訓練（training）
- 特徴量（feature）
- 目的変数（target）
- 損失関数（loss function）

### 1.4 機械学習システムを構築するためのロードマップ

機械学習を用いて予測モデルを構築する際の、一般的なワークフローについて

#### 1.4.1 前処理：データ整形

前処理（preprocessing）

- 生データが、機械学習アルゴリズムの性能を最適化するために必要な形式で提供されることは滅多にない

> 多くの機械学習アルゴリズムでは、最適な性能を得るために、選択した特徴量の尺度が同じであることも求められる。

例えば、特徴量を

- `[0, 1]`の範囲に変換する
- 平均が0、分散が1の標準正規分布に変換する

ことによって実現する。

#### 1.4.2 予測モデルの訓練と選択

- ノーフリーランチ定理（no-free-lunch theorem、NFL）
- 交差検証（cross-validation）
- 汎化性能（generalization performance）
- ハイパーパラメータ最適化（hyperparameter optimization）

#### 1.4.3 モデルの評価と未知のインスタンスの予測

### 1.5 機械学習に Python を使う

- NumPy、SciPy：多次元配列に対して高速にベクトル演算を行う拡張ライブラリ
- scikit-learn：最もよく使われている機械学習ライブラリの1つ
- TensolFlow：グラフィックボードを使用し、効率よくディープニューラルネットワークモデルの訓練を行えるライブラリ

#### 1.5.1 Python と Python Package Index のパッケージのインストール

本書では、Python 3.7 以降を対象

#### 1.5.2 Anaconda とパッケージマネージャの使用

#### 1.5.3 科学計算、データサイエンス、機械学習のパッケージ

- pandas：NumPyをベースにした、より高レベルなデータ操作を提供するライブラリ
- matplotlib：学習経験を高め、数値のデータを可視化するために、高度なカスタマイズが可能なライブラリ

### まとめ

---

## 第2章 分類問題 - 単純な機械学習アルゴリズムの訓練

以下のアルゴリズムを分類問題に適用する：

1. パーセプトロン
   - Pythonで実装する
   - Iris データセットに含まれているアヤメの花の品種を分類するよう訓練する
2. ADALINE (Adaptive Linear Neuron)
   - 最適化の基礎
   - scikit-learnを使った洗練された分類器を操作するための下準備

→ 機械学習のアルゴリズムの話！

取り上げる内容：

- 機械学習のアルゴリズム
- 以下を使った、データの読み込み・処理・可視化
  - pandas, NumPy, matplotlib
- Python による、線形分類の実装 

### 2.1 人工ニューロン - 機械学習の前史

McCulloch-Pitts ニューロン（MCP ニューロン）：簡略化された脳細胞に関する初めての概念

- 神経細胞を、ニ値出力を行う単純な論理ゲートと表現した
  - 複数の信号が樹状突起に届き、細胞体に取り込まれる
  - 蓄積された信号が特定の閾値を超えた場合、出力信号が生成され、軸索によって伝達される

→ ニューロンモデルに基づいて、パーセプトロンの学習規則を発表

- 最適な重み係数を自動的に学習
- 入力信号と掛け合わせ、ニューロンが発火（信号を伝達）するかどうかを判断

#### 2.1.1 人工ニューロンの正式な定義

人工ニューロン = 二値分類タスク（陽クラス = 1 / 陰クラス = -1）

$\mathbf{x}$ : 入力値（訓練データ）,
$\mathbf{w}$ : 重みベクトル
とする：

$$
\mathbf{x} = \begin{pmatrix} x_1 &  \cdots & x_m \end{pmatrix}^T
$$

$$
\mathbf{w} = \begin{pmatrix} w_1 & \cdots & w_m \end{pmatrix}
$$

これらの線型結合を $\mathbf{z}$ とする：

$$
\mathbf{z} = w_1x_1 +...+ w_mx_m
$$

$\theta$ を閾値とした場合、以下の関数 $\phi$ を決定関数という：

$$
\phi(z) = \begin{cases}
1 & \text{if } z \geq \theta \\
-1 & \text{if } z < \theta
\end{cases}
$$

線形代数の基礎：ドット積と行列の転置

#### 2.1.2 パーセプトロンの学習規則

### 2.2 パーセプトロンの学習アルゴリズムを Python で実装する

#### 2.2.1 オブジェクト指向のパーセプトロン API

### 2.3 Iris データセットでのパーセプトロンモデルの訓練

### 2.4 ADALINE と学習の収束

ADALINE (Adaptive Linear Neuron)

- 単層ニューラルネットワーク
- パーセプトロンアルゴリズムの改良と見なせる

### 2.5 勾配降下法によるコスト関数の最小化

cf.

- [勾配降下法とは何か？図解でわかりやすくざっくり解説!](https://aiwannabe.com/gradient-descent-explained/)
  - gradient descent
  - 最適化手法の1つ
  - 関数の最小値（最大値）を見つけるための手法
- [凸関数と凹関数](http://www.allisone.co.jp/html/Notes/Mathematics/Function/convex-concave-function/index.html)
- [勾配降下法](https://axa.biopapyrus.jp/deep-learning/gradient_descent_method.html)

#### 2.5.1 ADALINE と Python で実装する

#### 2.5.2 特徴量のスケーリングを通じて勾配降下法を改善する

### 2.6 大規模な機械学習と確率的勾配降下法

### まとめ

---

## 感想

- 割と最初から線形代数なり凸関数なり、すごい数学じゃん、いいんかんじ
